{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d72026-e696-4b7b-9255-2db4369ee878",
   "metadata": {},
   "source": [
    "## Neural Network for MNIST Data Set  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d510f-4edf-41dc-aa87-1ece4bdfd269",
   "metadata": {},
   "source": [
    "#### Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc51312-0a64-4d00-90c5-d35d2426657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "105190aa-f092-4d3f-bbc3-b7716ec35224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab62ce48-d77d-4971-af7f-247c5b30cf7a",
   "metadata": {},
   "source": [
    "#### Get MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3a705-6648-493c-8245-df7e779b186e",
   "metadata": {},
   "source": [
    "Obtain the MNIST dataset from tensorflow_datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f893568-c9f2-4eb6-bb31-7bd8403577a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ugur_\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f8e740a-0b10-4b4f-bf86-0b9390afdd31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "# load the dataset into the duo \"mnist_dataset, mnist_info\" by using the tfds.load method with \"with_info=True\" as an argument\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c33fa-3353-4125-9b40-351af6acdb26",
   "metadata": {},
   "source": [
    "\"as_supervised\": bool \n",
    "if True, the returned tf.data.Dataset will have a 2-tuple structure (input, label) according to builder.info.supervised_keys. If False, the default, the returned tf.data.Dataset will have a dictionary with all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b66bb-1b6b-4b11-9edd-8eb478665054",
   "metadata": {},
   "source": [
    "Dataset mnist downloaded and prepared to \"ABSOLUTE PATH\"\\tensorflow_datasets\\mnist\\1.0.0. \n",
    "Subsequent calls will reuse this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "941db6a9-1267-4690-b79e-f119febc0f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_OptionsDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train, mnist_test = mnist_dataset[\"train\"], mnist_dataset[\"test\"]\n",
    "mnist_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d77bc04-8729-44ef-959e-89cb61d78cdc",
   "metadata": {},
   "source": [
    "#### Validation Set Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b3a3552-e64c-46b0-b9a8-f12efb8ac374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_info.splits[\"train\"].num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf8917-a89e-4833-b889-44c233bffc74",
   "metadata": {},
   "source": [
    "Take 5 percent of training data to be validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f22f7504-fe8d-43ec-836e-15063256c679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=3000>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_validation_samples = tf.cast(0.05*mnist_info.splits[\"train\"].num_examples, tf.int64)\n",
    "num_validation_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54c24462-fc27-4940-bc6c-88b96ac46f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=10000>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_test_samples = tf.cast(mnist_info.splits[\"test\"].num_examples, tf.int64)\n",
    "num_test_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ae136-5730-400d-92fc-e6d94a61d407",
   "metadata": {},
   "source": [
    "#### Scaling the inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2847908f-4369-4738-936b-98054bc9eed2",
   "metadata": {},
   "source": [
    "Scale the data by \n",
    "1) defining the scale in a function, and then\n",
    "2) mapping the new values that are scaled onto their non-scaled value, with the \"map\" method of \"_OptionsDataset\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44492425-3aca-4dd6-b18f-d99d3549bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image,label):\n",
    "    image = tf.cast(image,tf.float32)\n",
    "    image = float(image / 255)\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffd8ee11-dc14-4123-9c85-03c2e38866d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_and_validation_data = mnist_train.map(scale) \n",
    "scaled_test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101aa070-f410-4499-b2ab-f393f3d89b9d",
   "metadata": {},
   "source": [
    "#### Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "215e4eb6-f609-4975-843d-547f618622df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method shuffle in module tensorflow.python.data.ops.dataset_ops:\n",
      "\n",
      "shuffle(buffer_size, seed=None, reshuffle_each_iteration=None, name=None) method of tensorflow.python.data.ops.dataset_ops._OptionsDataset instance\n",
      "    Randomly shuffles the elements of this dataset.\n",
      "    \n",
      "    This dataset fills a buffer with `buffer_size` elements, then randomly\n",
      "    samples elements from this buffer, replacing the selected elements with new\n",
      "    elements. For perfect shuffling, a buffer size greater than or equal to the\n",
      "    full size of the dataset is required.\n",
      "    \n",
      "    For instance, if your dataset contains 10,000 elements but `buffer_size` is\n",
      "    set to 1,000, then `shuffle` will initially select a random element from\n",
      "    only the first 1,000 elements in the buffer. Once an element is selected,\n",
      "    its space in the buffer is replaced by the next (i.e. 1,001-st) element,\n",
      "    maintaining the 1,000 element buffer.\n",
      "    \n",
      "    `reshuffle_each_iteration` controls whether the shuffle order should be\n",
      "    different for each epoch. In TF 1.X, the idiomatic way to create epochs\n",
      "    was through the `repeat` transformation:\n",
      "    \n",
      "    ```python\n",
      "    dataset = tf.data.Dataset.range(3)\n",
      "    dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
      "    dataset = dataset.repeat(2)\n",
      "    # [1, 0, 2, 1, 2, 0]\n",
      "    \n",
      "    dataset = tf.data.Dataset.range(3)\n",
      "    dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
      "    dataset = dataset.repeat(2)\n",
      "    # [1, 0, 2, 1, 0, 2]\n",
      "    ```\n",
      "    \n",
      "    In TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it\n",
      "    possible to also create epochs through Python iteration:\n",
      "    \n",
      "    ```python\n",
      "    dataset = tf.data.Dataset.range(3)\n",
      "    dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
      "    list(dataset.as_numpy_iterator())\n",
      "    # [1, 0, 2]\n",
      "    list(dataset.as_numpy_iterator())\n",
      "    # [1, 2, 0]\n",
      "    ```\n",
      "    \n",
      "    ```python\n",
      "    dataset = tf.data.Dataset.range(3)\n",
      "    dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
      "    list(dataset.as_numpy_iterator())\n",
      "    # [1, 0, 2]\n",
      "    list(dataset.as_numpy_iterator())\n",
      "    # [1, 0, 2]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      "        elements from this dataset from which the new dataset will sample.\n",
      "      seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
      "        seed that will be used to create the distribution. See\n",
      "        `tf.random.set_seed` for behavior.\n",
      "      reshuffle_each_iteration: (Optional.) A boolean, which if true indicates\n",
      "        that the dataset should be pseudorandomly reshuffled each time it is\n",
      "        iterated over. (Defaults to `True`.)\n",
      "      name: (Optional.) A name for the tf.data operation.\n",
      "    \n",
      "    Returns:\n",
      "      Dataset: A `Dataset`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(mnist_train.shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de20a56f-6d6c-4432-a264-ae859077fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa1512c-ef15-424b-9153-607595e6cc0a",
   "metadata": {},
   "source": [
    "#### Seperate validation and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a05f9a8-a210-4400-8304-d167e4682e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "training_data = shuffled_train_and_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb1d75-0b23-4385-a516-2c70288c9319",
   "metadata": {},
   "source": [
    "#### Minibatch Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f14e8983-c790-4bdd-936e-6d3ba576e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b4a42d5-ff4a-4bb9-aad7-dca65e847e6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method batch in module tensorflow.python.data.ops.dataset_ops:\n",
      "\n",
      "batch(batch_size, drop_remainder=False, num_parallel_calls=None, deterministic=None, name=None) method of tensorflow.python.data.ops.dataset_ops.SkipDataset instance\n",
      "    Combines consecutive elements of this dataset into batches.\n",
      "    \n",
      "    >>> dataset = tf.data.Dataset.range(8)\n",
      "    >>> dataset = dataset.batch(3)\n",
      "    >>> list(dataset.as_numpy_iterator())\n",
      "    [array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n",
      "    \n",
      "    >>> dataset = tf.data.Dataset.range(8)\n",
      "    >>> dataset = dataset.batch(3, drop_remainder=True)\n",
      "    >>> list(dataset.as_numpy_iterator())\n",
      "    [array([0, 1, 2]), array([3, 4, 5])]\n",
      "    \n",
      "    The components of the resulting element will have an additional outer\n",
      "    dimension, which will be `batch_size` (or `N % batch_size` for the last\n",
      "    element if `batch_size` does not divide the number of input elements `N`\n",
      "    evenly and `drop_remainder` is `False`). If your program depends on the\n",
      "    batches having the same outer dimension, you should set the `drop_remainder`\n",
      "    argument to `True` to prevent the smaller batch from being produced.\n",
      "    \n",
      "    Note: If your program requires data to have a statically known shape (e.g.,\n",
      "    when using XLA), you should use `drop_remainder=True`. Without\n",
      "    `drop_remainder=True` the shape of the output dataset will have an unknown\n",
      "    leading dimension due to the possibility of a smaller final batch.\n",
      "    \n",
      "    Args:\n",
      "      batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      "        consecutive elements of this dataset to combine in a single batch.\n",
      "      drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      "        whether the last batch should be dropped in the case it has fewer than\n",
      "        `batch_size` elements; the default behavior is not to drop the smaller\n",
      "        batch.\n",
      "      num_parallel_calls: (Optional.) A `tf.int64` scalar `tf.Tensor`,\n",
      "        representing the number of batches to compute asynchronously in\n",
      "        parallel.\n",
      "        If not specified, batches will be computed sequentially. If the value\n",
      "        `tf.data.AUTOTUNE` is used, then the number of parallel\n",
      "        calls is set dynamically based on available resources.\n",
      "      deterministic: (Optional.) When `num_parallel_calls` is specified, if this\n",
      "        boolean is specified (`True` or `False`), it controls the order in which\n",
      "        the transformation produces elements. If set to `False`, the\n",
      "        transformation is allowed to yield elements out of order to trade\n",
      "        determinism for performance. If not specified, the\n",
      "        `tf.data.Options.deterministic` option (`True` by default) controls the\n",
      "        behavior.\n",
      "      name: (Optional.) A name for the tf.data operation.\n",
      "    \n",
      "    Returns:\n",
      "      Dataset: A `Dataset`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(training_data.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2f9e8f9-0437-44a9-9325-da30072b534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.batch(BATCH_SIZE)\n",
    "# after training with each batch, we will calculate loss with the validation dataset\n",
    "# hence, the validation dataset doesn't need to be split into batches, as we will use all of it to just forward-feed and \n",
    "# calculate total loss over it\n",
    "validation_data = validation_data.batch(num_validation_samples) # still, the model expects all the data to be in batch form\n",
    "test_data = scaled_test_data.batch(num_test_samples)\n",
    "# after we see an increase in the total loss over the validation set, we will stop and test the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665e2a7-8151-4c67-aed5-1730e751ee21",
   "metadata": {},
   "source": [
    "The validation data must have the same shape and object properties as the train and test data.\n",
    "\n",
    "The MNIST data is iterable and in 2-tuple format, because we loaded the data (with .load method of tfds) with \n",
    "\"as_supervised=True\".\n",
    "\n",
    "Hence,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83bcaec1-5eab-4e90-a546-a278d776e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_inputs, validation_targets = next(iter(validation_data)) \n",
    "# iter turns the data to iterable form\n",
    "# next function loads the next element of an iterable object "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dde9e3-8b49-4835-a17e-6999067a5d22",
   "metadata": {},
   "source": [
    "#### Model Formation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de44c99b-1005-46e5-96db-c65745133841",
   "metadata": {},
   "source": [
    "We have 28-by-28 images of numbers from 0 to 1, which were scaled from the original values of 0 to 255.\n",
    "We are estimating which number each image shows, and there are 10 possible numbers that can come out. \n",
    "\n",
    "Hence,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "090f0647-c389-4aed-a1bf-346f134bde7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6765523e-42af-4854-9e6f-7267c6062c3c",
   "metadata": {},
   "source": [
    "Now we set up the layers of the model. The lecturer tells us to put two hidden layers, and both shall contain 50 nodes.\n",
    "\n",
    "The lecturer also tells that this choice is definitely suboptimal, and gives a homework to vary these hyperparameters for more accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f71f7be2-723d-4666-aa36-dd149cba739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fae4d1-a3a8-4050-b4c6-0957280030c1",
   "metadata": {},
   "source": [
    "Now the time to set the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c876e849-a9be-4ea0-8091-135a43e592fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)), # the input layer\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"), # first hidden layer\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"), # second hidden layer\n",
    "                            tf.keras.layers.Dense(output_size, activation=\"softmax\") # the output layer\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933cdd11-4340-4e8c-af5e-5122262e87f3",
   "metadata": {},
   "source": [
    "#### Choosing the optimizer and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21bbbcc8-f958-4575-b7da-ce844bc76dc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method compile in module keras.engine.training:\n",
      "\n",
      "compile(optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, steps_per_execution=None, jit_compile=None, **kwargs) method of keras.engine.sequential.Sequential instance\n",
      "    Configures the model for training.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    ```python\n",
      "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
      "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
      "                  metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
      "                           tf.keras.metrics.FalseNegatives()])\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "        optimizer: String (name of optimizer) or optimizer instance. See\n",
      "          `tf.keras.optimizers`.\n",
      "        loss: Loss function. May be a string (name of loss function), or\n",
      "          a `tf.keras.losses.Loss` instance. See `tf.keras.losses`. A loss\n",
      "          function is any callable with the signature `loss = fn(y_true,\n",
      "          y_pred)`, where `y_true` are the ground truth values, and\n",
      "          `y_pred` are the model's predictions.\n",
      "          `y_true` should have shape\n",
      "          `(batch_size, d0, .. dN)` (except in the case of\n",
      "          sparse loss functions such as\n",
      "          sparse categorical crossentropy which expects integer arrays of\n",
      "          shape `(batch_size, d0, .. dN-1)`).\n",
      "          `y_pred` should have shape `(batch_size, d0, .. dN)`.\n",
      "          The loss function should return a float tensor.\n",
      "          If a custom `Loss` instance is\n",
      "          used and reduction is set to `None`, return value has shape\n",
      "          `(batch_size, d0, .. dN-1)` i.e. per-sample or per-timestep loss\n",
      "          values; otherwise, it is a scalar. If the model has multiple\n",
      "          outputs, you can use a different loss on each output by passing a\n",
      "          dictionary or a list of losses. The loss value that will be\n",
      "          minimized by the model will then be the sum of all individual\n",
      "          losses, unless `loss_weights` is specified.\n",
      "        metrics: List of metrics to be evaluated by the model during\n",
      "          training and testing. Each of this can be a string (name of a\n",
      "          built-in function), function or a `tf.keras.metrics.Metric`\n",
      "          instance. See `tf.keras.metrics`. Typically you will use\n",
      "          `metrics=['accuracy']`.\n",
      "          A function is any callable with the signature `result = fn(y_true,\n",
      "          y_pred)`. To specify different metrics for different outputs of a\n",
      "          multi-output model, you could also pass a dictionary, such as\n",
      "          `metrics={'output_a':'accuracy', 'output_b':['accuracy', 'mse']}`.\n",
      "          You can also pass a list to specify a metric or a list of metrics\n",
      "          for each output, such as\n",
      "          `metrics=[['accuracy'], ['accuracy', 'mse']]`\n",
      "          or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
      "          strings 'accuracy' or 'acc', we convert this to one of\n",
      "          `tf.keras.metrics.BinaryAccuracy`,\n",
      "          `tf.keras.metrics.CategoricalAccuracy`,\n",
      "          `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n",
      "          function used and the model output shape. We do a similar\n",
      "          conversion for the strings 'crossentropy' and 'ce' as well.\n",
      "          The metrics passed here are evaluated without sample weighting; if\n",
      "          you would like sample weighting to apply, you can specify your\n",
      "          metrics via the `weighted_metrics` argument instead.\n",
      "        loss_weights: Optional list or dictionary specifying scalar\n",
      "          coefficients (Python floats) to weight the loss contributions of\n",
      "          different model outputs. The loss value that will be minimized by\n",
      "          the model will then be the *weighted sum* of all individual\n",
      "          losses, weighted by the `loss_weights` coefficients.  If a list,\n",
      "          it is expected to have a 1:1 mapping to the model's outputs. If a\n",
      "          dict, it is expected to map output names (strings) to scalar\n",
      "          coefficients.\n",
      "        weighted_metrics: List of metrics to be evaluated and weighted by\n",
      "          `sample_weight` or `class_weight` during training and testing.\n",
      "        run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n",
      "          logic will not be wrapped in a `tf.function`. Recommended to leave\n",
      "          this as `None` unless your `Model` cannot be run inside a\n",
      "          `tf.function`. `run_eagerly=True` is not supported when using\n",
      "          `tf.distribute.experimental.ParameterServerStrategy`.\n",
      "        steps_per_execution: Int. Defaults to 1. The number of batches to\n",
      "          run during each `tf.function` call. Running multiple batches\n",
      "          inside a single `tf.function` call can greatly improve performance\n",
      "          on TPUs or small models with a large Python overhead. At most, one\n",
      "          full epoch will be run each execution. If a number larger than the\n",
      "          size of the epoch is passed, the execution will be truncated to\n",
      "          the size of the epoch. Note that if `steps_per_execution` is set\n",
      "          to `N`, `Callback.on_batch_begin` and `Callback.on_batch_end`\n",
      "          methods will only be called every `N` batches (i.e. before/after\n",
      "          each `tf.function` execution).\n",
      "        jit_compile: If `True`, compile the model training step with XLA.\n",
      "          [XLA](https://www.tensorflow.org/xla) is an optimizing compiler\n",
      "          for machine learning.\n",
      "          `jit_compile` is not enabled for by default.\n",
      "          This option cannot be enabled with `run_eagerly=True`.\n",
      "          Note that `jit_compile=True`\n",
      "          may not necessarily work for all models.\n",
      "          For more information on supported operations please refer to the\n",
      "          [XLA documentation](https://www.tensorflow.org/xla).\n",
      "          Also refer to\n",
      "          [known XLA issues](https://www.tensorflow.org/xla/known_issues)\n",
      "          for more details.\n",
      "        **kwargs: Arguments supported for backwards compatibility only.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.compile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a5bb9c-c7e4-42e2-afe1-4f20a541a8e5",
   "metadata": {},
   "source": [
    "For loss function, we choose \"sparse_categorical_crossentropy\" because:\n",
    "\n",
    "1) Binary crossentropy is for cases when categories are base 2 numbers, which is not valid for the outputs here.\n",
    "2) Categorical crossentropy expects the outputs to be one-hot encoded, and our outputs are not one-hot encoded.\n",
    "3) Sparse Categorical crossentropy applies one-hot encoding to the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17471d-bc8e-4c3b-96a8-a01059d1be90",
   "metadata": {},
   "source": [
    "Additionally, we can put in types of metrics to be calculated into the compile method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd9c3a67-d99f-4b61-a420-ba77060af46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff81882-070f-4304-9575-f6b1836d0062",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26ab5bd5-b16c-42d5-9012-0f63bde4bd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1140/1140 - 5s - loss: 0.2448 - accuracy: 0.9282 - val_loss: 0.1499 - val_accuracy: 0.9553 - 5s/epoch - 4ms/step\n",
      "Epoch 2/8\n",
      "1140/1140 - 3s - loss: 0.1001 - accuracy: 0.9692 - val_loss: 0.0980 - val_accuracy: 0.9717 - 3s/epoch - 3ms/step\n",
      "Epoch 3/8\n",
      "1140/1140 - 3s - loss: 0.0680 - accuracy: 0.9789 - val_loss: 0.0801 - val_accuracy: 0.9750 - 3s/epoch - 3ms/step\n",
      "Epoch 4/8\n",
      "1140/1140 - 3s - loss: 0.0499 - accuracy: 0.9847 - val_loss: 0.0546 - val_accuracy: 0.9793 - 3s/epoch - 3ms/step\n",
      "Epoch 5/8\n",
      "1140/1140 - 3s - loss: 0.0409 - accuracy: 0.9871 - val_loss: 0.0462 - val_accuracy: 0.9847 - 3s/epoch - 3ms/step\n",
      "Epoch 6/8\n",
      "1140/1140 - 3s - loss: 0.0330 - accuracy: 0.9892 - val_loss: 0.0411 - val_accuracy: 0.9853 - 3s/epoch - 3ms/step\n",
      "Epoch 7/8\n",
      "1140/1140 - 3s - loss: 0.0277 - accuracy: 0.9905 - val_loss: 0.0319 - val_accuracy: 0.9877 - 3s/epoch - 3ms/step\n",
      "Epoch 8/8\n",
      "1140/1140 - 3s - loss: 0.0219 - accuracy: 0.9924 - val_loss: 0.0296 - val_accuracy: 0.9907 - 3s/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22bee910940>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 8\n",
    "\n",
    "model.fit(training_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs,validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe851484-c82a-4e9e-97ef-1c4806cfa107",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "129e7c61-f77a-4321-bd29-ef1d187017e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0834 - accuracy: 0.9793\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f3b3043-7b0c-4fbd-a443-1c53aab02a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08, Test accuracy: 97.93%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {0:.2f}, Test accuracy: {1:.2f}%\".format(test_loss,test_accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
